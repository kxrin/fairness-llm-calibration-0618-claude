# Fairness-Aware Calibration of LLM Evaluators Using Tokenized Disclosures

## Overview

This is a fork of the original [Hypogenic-AI baseline study](https://github.com/Hypogenic-AI/fairness-llm-calibration-0618-claude), which was generated using the [Idea Explorer](https://github.com/ChicagoHAI/idea-explorer) autonomous research framework by H. Liu and C. Tan. The original study found that GPT-4.1 penalizes AI-disclosed text by -0.100 points (p=0.003, d=-0.31) and tested several prompt-based calibration strategies.

I identified areas where the original work could be improved and implemented three extended investigations that build on the baseline findings. My suggestions, implementations, and results are documented in [REPORT_IMPROVED.md](REPORT_IMPROVED.md).

## Reports

- [REPORT.md](REPORT.md) — Original baseline report (generated by Idea Explorer)
- [REPORT_IMPROVED.md](REPORT_IMPROVED.md) — My suggested improvements, implementations, and full results

## How to Reproduce

### Environment Setup
```bash
uv venv
source .venv/bin/activate
uv pip install pyarrow numpy pandas matplotlib scipy scikit-learn openai datasets tqdm seaborn statsmodels httpx
```

### Running Experiments
```bash
export OPENAI_API_KEY=your_key_here

# original baseline experiments
cd src
python run_experiments_fast.py
python analysis.py
python analysis_extended.py

# my extended investigations
python run_position_experiment.py
python run_logprobs_experiment.py
python analysis_reasoning.py
```

### File Structure
```
.
├── REPORT.md                    # original baseline report
├── REPORT_IMPROVED.md           # my improvements and extended analysis
├── README.md                    
├── src/
│   ├── config.py                
│   ├── data_prep.py             
│   ├── evaluator.py             
│   ├── run_experiments_fast.py  
│   ├── run_position_experiment.py   
│   ├── run_logprobs_experiment.py   
│   ├── analysis.py              
│   ├── analysis_extended.py     
│   └── analysis_reasoning.py    
├── results/                     #results and plots
├── datasets/                    # datasets
├── papers/                      
└── code/                        
```

## References

- [1] X. Wu, Y. Wang, S. Jegelka, and A. Jadbabaie, "On the Emergence of Position Bias in Transformers," *arXiv*, 2025. https://arxiv.org/abs/2502.01951
- [2] H. Liu and C. Tan, "Idea Explorer: Autonomous Research Framework," 2025. https://github.com/ChicagoHAI/idea-explorer
